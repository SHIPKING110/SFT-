# scripts/evaluate_model.py
import os
import json
import torch
import pandas as pd
import numpy as np
from datetime import datetime
from typing import List, Dict, Any
from transformers import (
    AutoModelForCausalLM,
    AutoTokenizer,
    pipeline
)

class ModelEvaluator:
    def __init__(self, model_path: str, tokenizer_path: str = None):
        """åˆå§‹åŒ–è¯„æµ‹å™¨"""
        self.model_path = model_path
        self.tokenizer_path = tokenizer_path or model_path
        
        print(f"åŠ è½½æ¨¡å‹: {model_path}")
        self.load_model()
        
        # åˆå§‹åŒ–æŒ‡æ ‡è®¡ç®—å™¨ï¼ˆé¿å…ä¾èµ–é—®é¢˜ï¼‰
        self.metrics_initialized = False
        self.init_metrics()
    
    def init_metrics(self):
        """åˆå§‹åŒ–è¯„ä¼°æŒ‡æ ‡ï¼Œå¤„ç†ä¾èµ–é—®é¢˜"""
        try:
            import evaluate
            self.bleu_metric = evaluate.load("bleu")
            self.rouge_metric = evaluate.load("rouge")
            self.metrics_initialized = True
            print("âœ… è¯„ä¼°æŒ‡æ ‡åŠ è½½æˆåŠŸ")
        except ImportError as e:
            print(f"âš ï¸  è¯„ä¼°æŒ‡æ ‡ä¾èµ–ç¼ºå¤±: {e}")
            print("ğŸ”§ è¯·å®‰è£…ä¾èµ–: pip install evaluate rouge-score nltk absl-py")
            self.metrics_initialized = False
        except Exception as e:
            print(f"âš ï¸  è¯„ä¼°æŒ‡æ ‡åˆå§‹åŒ–å¤±è´¥: {e}")
            self.metrics_initialized = False
    
    def load_model(self):
        """åŠ è½½æ¨¡å‹å’Œtokenizer"""
        try:
            self.tokenizer = AutoTokenizer.from_pretrained(
                self.tokenizer_path, 
                trust_remote_code=True
            )
            
            if self.tokenizer.pad_token is None:
                self.tokenizer.pad_token = self.tokenizer.eos_token
                
            self.model = AutoModelForCausalLM.from_pretrained(
                self.model_path,
                torch_dtype=torch.float16,
                device_map="auto",
                trust_remote_code=True
            )
            
            print("âœ… æ¨¡å‹åŠ è½½æˆåŠŸ")
            
        except Exception as e:
            print(f"âŒ æ¨¡å‹åŠ è½½å¤±è´¥: {e}")
            raise
    
    def generate_response(self, prompt: str, max_length: int = 512) -> str:
        """ç”Ÿæˆå›å¤"""
        try:
            # ç›´æ¥ä½¿ç”¨promptï¼Œä¸åº”ç”¨chatæ¨¡æ¿
            inputs = self.tokenizer(prompt, return_tensors="pt").to(self.model.device)
            
            # ç”Ÿæˆå›å¤
            with torch.no_grad():
                outputs = self.model.generate(
                    **inputs,
                    max_new_tokens=max_length,
                    do_sample=True,
                    temperature=0.7,
                    top_p=0.9,
                    pad_token_id=self.tokenizer.pad_token_id,
                    eos_token_id=self.tokenizer.eos_token_id,
                    repetition_penalty=1.1
                )
            
            # è§£ç å›å¤
            response = self.tokenizer.decode(
                outputs[0][len(inputs['input_ids'][0]):], 
                skip_special_tokens=True
            )
            
            return response.strip()
            
        except Exception as e:
            print(f"ç”Ÿæˆå›å¤æ—¶å‡ºé”™: {e}")
            return ""
    
    def extract_reasoning_and_answer(self, response: str) -> tuple:
        """ä»å›å¤ä¸­æå–æ¨ç†è¿‡ç¨‹å’Œæœ€ç»ˆç­”æ¡ˆ"""
        reasoning = ""
        answer = ""
        
        if "<reasoning>" in response and "</reasoning>" in response:
            # æå–æ¨ç†éƒ¨åˆ†
            start_idx = response.find("<reasoning>") + len("<reasoning>")
            end_idx = response.find("</reasoning>")
            reasoning = response[start_idx:end_idx].strip()
            
            # æå–ç­”æ¡ˆéƒ¨åˆ†
            answer_start = response.find("ç­”ï¼š")
            if answer_start != -1:
                answer = response[answer_start + len("ç­”ï¼š"):].strip()
        else:
            # å¦‚æœæ²¡æœ‰ç‰¹å®šæ ¼å¼ï¼Œå°è¯•åˆ†å‰²
            if "ç­”ï¼š" in response:
                answer_start = response.find("ç­”ï¼š")
                reasoning = response[:answer_start].strip()
                answer = response[answer_start + len("ç­”ï¼š"):].strip()
            else:
                answer = response
        
        return reasoning, answer
    
    def calculate_similarity_metrics(self, reference: str, prediction: str) -> Dict:
        """è®¡ç®—ç›¸ä¼¼åº¦æŒ‡æ ‡ï¼ˆç®€åŒ–ç‰ˆï¼Œé¿å…ä¾èµ–é—®é¢˜ï¼‰"""
        metrics = {}
        
        if not self.metrics_initialized:
            # ä½¿ç”¨ç®€å•çš„å­—ç¬¦ä¸²åŒ¹é…ä½œä¸ºå¤‡é€‰
            metrics.update(self.calculate_basic_metrics(reference, prediction))
            return metrics
        
        try:
            # BLEU score
            bleu_result = self.bleu_metric.compute(
                predictions=[prediction],
                references=[[reference]]
            )
            metrics["bleu"] = float(bleu_result["bleu"])  # è½¬æ¢ä¸ºPython float
            
            # ROUGE score
            rouge_result = self.rouge_metric.compute(
                predictions=[prediction],
                references=[reference]
            )
            metrics["rouge1"] = float(rouge_result["rouge1"])  # è½¬æ¢ä¸ºPython float
            metrics["rouge2"] = float(rouge_result["rouge2"])  # è½¬æ¢ä¸ºPython float
            metrics["rougeL"] = float(rouge_result["rougeL"])  # è½¬æ¢ä¸ºPython float
            
        except Exception as e:
            print(f"è®¡ç®—ç›¸ä¼¼åº¦æŒ‡æ ‡æ—¶å‡ºé”™ï¼Œä½¿ç”¨åŸºç¡€æŒ‡æ ‡: {e}")
            metrics.update(self.calculate_basic_metrics(reference, prediction))
        
        return metrics
    
    def calculate_basic_metrics(self, reference: str, prediction: str) -> Dict:
        """è®¡ç®—åŸºç¡€ç›¸ä¼¼åº¦æŒ‡æ ‡ï¼ˆä¸ä¾èµ–å¤–éƒ¨åŒ…ï¼‰"""
        ref_words = set(reference.split())
        pred_words = set(prediction.split())
        
        # è®¡ç®—Jaccardç›¸ä¼¼åº¦
        intersection = len(ref_words.intersection(pred_words))
        union = len(ref_words.union(pred_words))
        jaccard_similarity = float(intersection / union if union > 0 else 0)
        
        # è®¡ç®—é‡å ç‡
        overlap_ratio = float(len([w for w in prediction.split() if w in reference]) / len(prediction.split()) if prediction else 0)
        
        return {
            "jaccard_similarity": jaccard_similarity,
            "overlap_ratio": overlap_ratio,
            "bleu": jaccard_similarity,  # ç”¨Jaccardè¿‘ä¼¼BLEU
            "rouge1": overlap_ratio,     # ç”¨é‡å ç‡è¿‘ä¼¼ROUGE-1
            "rouge2": 0.0,               # ç®€åŒ–å¤„ç†
            "rougeL": overlap_ratio      # ç”¨é‡å ç‡è¿‘ä¼¼ROUGE-L
        }
    
    def evaluate_single_example(self, test_case: Dict) -> Dict:
        """è¯„ä¼°å•ä¸ªæµ‹è¯•ç”¨ä¾‹"""
        question = test_case["input"].replace("ç”¨æˆ·ï¼š", "").strip()
        expected_output = test_case["output"]
        
        # ç”Ÿæˆå›å¤
        start_time = datetime.now()
        generated_response = self.generate_response(question)
        generation_time = (datetime.now() - start_time).total_seconds()
        
        # æå–æ¨ç†å’Œç­”æ¡ˆ
        reasoning, answer = self.extract_reasoning_and_answer(generated_response)
        
        # è®¡ç®—æŒ‡æ ‡
        similarity_metrics = self.calculate_similarity_metrics(expected_output, generated_response)
        
        # è®¡ç®—å“åº”é•¿åº¦
        response_length = len(generated_response)
        
        return {
            "question": question,
            "expected_output": expected_output,
            "generated_response": generated_response,
            "reasoning": reasoning,
            "answer": answer,
            "generation_time": float(generation_time),  # è½¬æ¢ä¸ºPython float
            "response_length": int(response_length),    # è½¬æ¢ä¸ºPython int
            "similarity_metrics": similarity_metrics,
            "has_reasoning_format": "<reasoning>" in generated_response and "</reasoning>" in generated_response,
            "has_answer_format": "ç­”ï¼š" in generated_response,
            "is_empty_response": len(generated_response.strip()) == 0
        }
    
    def evaluate_on_dataset(self, test_file: str, num_samples: int = None) -> Dict:
        """åœ¨æµ‹è¯•é›†ä¸Šè¿›è¡Œè¯„ä¼°"""
        print(f"å¼€å§‹è¯„ä¼°ï¼Œæµ‹è¯•æ–‡ä»¶: {test_file}")
        
        # æ£€æŸ¥æµ‹è¯•æ–‡ä»¶æ˜¯å¦å­˜åœ¨
        if not os.path.exists(test_file):
            raise FileNotFoundError(f"æµ‹è¯•æ–‡ä»¶ä¸å­˜åœ¨: {test_file}")
        
        # åŠ è½½æµ‹è¯•æ•°æ®
        test_data = []
        with open(test_file, 'r', encoding='utf-8') as f:
            for line in f:
                test_data.append(json.loads(line.strip()))
        
        if num_samples and num_samples < len(test_data):
            test_data = test_data[:num_samples]
        
        print(f"æµ‹è¯•æ ·æœ¬æ•°é‡: {len(test_data)}")
        
        results = []
        total_metrics = {
            "generation_time": [],
            "response_length": [],
            "has_reasoning_format": 0,
            "has_answer_format": 0,
            "empty_responses": 0
        }
        
        # åˆå§‹åŒ–æŒ‡æ ‡ç´¯è®¡
        metric_keys = ["bleu", "rouge1", "rouge2", "rougeL", "jaccard_similarity", "overlap_ratio"]
        for key in metric_keys:
            total_metrics[key] = []
        
        for i, test_case in enumerate(test_data):
            print(f"å¤„ç†æ ·æœ¬ {i+1}/{len(test_data)}")
            
            try:
                result = self.evaluate_single_example(test_case)
                results.append(result)
                
                # ç´¯è®¡æŒ‡æ ‡
                metrics = result["similarity_metrics"]
                for key in metrics:
                    if key in total_metrics:
                        total_metrics[key].append(metrics[key])
                
                # ç´¯è®¡å…¶ä»–æŒ‡æ ‡
                total_metrics["generation_time"].append(result["generation_time"])
                total_metrics["response_length"].append(result["response_length"])
                total_metrics["has_reasoning_format"] += int(result["has_reasoning_format"])
                total_metrics["has_answer_format"] += int(result["has_answer_format"])
                total_metrics["empty_responses"] += int(result["is_empty_response"])
                        
            except Exception as e:
                print(f"è¯„ä¼°æ ·æœ¬ {i+1} æ—¶å‡ºé”™: {e}")
                continue
        
        # è®¡ç®—å¹³å‡æŒ‡æ ‡ - ç¡®ä¿æ‰€æœ‰å€¼éƒ½æ˜¯PythonåŸç”Ÿç±»å‹
        avg_metrics = {}
        for key, values in total_metrics.items():
            if isinstance(values, list) and values:
                # è½¬æ¢ä¸ºPythonåŸç”Ÿç±»å‹
                avg_metrics[f"avg_{key}"] = float(np.mean(values))
                avg_metrics[f"std_{key}"] = float(np.std(values))
                avg_metrics[f"min_{key}"] = float(np.min(values))
                avg_metrics[f"max_{key}"] = float(np.max(values))
            else:
                # å¯¹äºéåˆ—è¡¨å€¼ï¼ˆè®¡æ•°ç±»å‹ï¼‰ï¼Œç›´æ¥ä½¿ç”¨
                avg_metrics[key] = values
        
        # è®¡ç®—æ ¼å¼æ­£ç¡®ç‡
        if results:
            avg_metrics["reasoning_format_rate"] = float(total_metrics["has_reasoning_format"] / len(results))
            avg_metrics["answer_format_rate"] = float(total_metrics["has_answer_format"] / len(results))
            avg_metrics["empty_response_rate"] = float(total_metrics["empty_responses"] / len(results))
            avg_metrics["total_samples"] = int(len(results))  # è½¬æ¢ä¸ºPython int
        else:
            avg_metrics.update({
                "reasoning_format_rate": 0.0,
                "answer_format_rate": 0.0,
                "empty_response_rate": 0.0,
                "total_samples": 0
            })
        
        return {
            "results": results,
            "summary": avg_metrics,
            "total_samples": len(results),
            "metrics_available": self.metrics_initialized
        }
    
    def convert_to_serializable(self, obj):
        """å°†å¯¹è±¡è½¬æ¢ä¸ºJSONå¯åºåˆ—åŒ–çš„æ ¼å¼"""
        if isinstance(obj, (np.integer, np.int64, np.int32)):
            return int(obj)
        elif isinstance(obj, (np.floating, np.float64, np.float32)):
            return float(obj)
        elif isinstance(obj, np.ndarray):
            return obj.tolist()
        elif isinstance(obj, dict):
            return {key: self.convert_to_serializable(value) for key, value in obj.items()}
        elif isinstance(obj, list):
            return [self.convert_to_serializable(item) for item in obj]
        else:
            return obj
    
    def run_comprehensive_evaluation(self, test_file: str, output_dir: str, num_samples: int = None):
        """è¿è¡Œç»¼åˆè¯„ä¼°"""
        print("å¼€å§‹ç»¼åˆè¯„ä¼°...")
        
        # åˆ›å»ºè¾“å‡ºç›®å½•
        os.makedirs(output_dir, exist_ok=True)
        
        # è¯„ä¼°ç»“æœ
        evaluation_result = self.evaluate_on_dataset(test_file, num_samples)
        
        # ä¿å­˜è¯¦ç»†ç»“æœ
        timestamp = datetime.now().strftime("%Y%m%d_%H%M%S")
        
        # è½¬æ¢ä¸ºå¯åºåˆ—åŒ–çš„æ ¼å¼
        serializable_result = self.convert_to_serializable(evaluation_result)
        
        # ä¿å­˜è¯¦ç»†ç»“æœ
        detailed_file = os.path.join(output_dir, f"detailed_results_{timestamp}.json")
        with open(detailed_file, 'w', encoding='utf-8') as f:
            json.dump(serializable_result, f, indent=2, ensure_ascii=False)
        
        # ä¿å­˜æ‘˜è¦ç»“æœ
        summary_file = os.path.join(output_dir, f"summary_{timestamp}.json")
        with open(summary_file, 'w', encoding='utf-8') as f:
            json.dump(serializable_result["summary"], f, indent=2, ensure_ascii=False)
        
        # ç”ŸæˆCSVæŠ¥å‘Š
        self.generate_csv_report(serializable_result["results"], output_dir, timestamp)
        
        # æ‰“å°è¯„ä¼°æ‘˜è¦
        self.print_evaluation_summary(serializable_result["summary"], serializable_result["metrics_available"])
        
        print(f"âœ… è¯„ä¼°å®Œæˆ!")
        print(f"ğŸ“Š è¯¦ç»†ç»“æœ: {detailed_file}")
        print(f"ğŸ“ˆ è¯„ä¼°æ‘˜è¦: {summary_file}")
        
        return evaluation_result
    
    def generate_csv_report(self, results: List[Dict], output_dir: str, timestamp: str):
        """ç”ŸæˆCSVæ ¼å¼çš„æŠ¥å‘Š"""
        csv_data = []
        
        for i, result in enumerate(results):
            row = {
                "id": i + 1,
                "question": result["question"],
                "expected_output": result["expected_output"],
                "generated_response": result["generated_response"],
                "reasoning": result["reasoning"],
                "answer": result["answer"],
                "generation_time": result["generation_time"],
                "response_length": result["response_length"],
                "has_reasoning_format": result["has_reasoning_format"],
                "has_answer_format": result["has_answer_format"],
                "is_empty_response": result["is_empty_response"]
            }
            
            # æ·»åŠ ç›¸ä¼¼åº¦æŒ‡æ ‡
            for metric_name, value in result["similarity_metrics"].items():
                row[metric_name] = value
            
            csv_data.append(row)
        
        csv_file = os.path.join(output_dir, f"evaluation_report_{timestamp}.csv")
        df = pd.DataFrame(csv_data)
        df.to_csv(csv_file, index=False, encoding='utf-8')
        print(f"ğŸ“‹ CSVæŠ¥å‘Š: {csv_file}")
    
    def print_evaluation_summary(self, summary: Dict, metrics_available: bool):
        """æ‰“å°è¯„ä¼°æ‘˜è¦"""
        print("\n" + "="*60)
        print("ğŸ“Š æ¨¡å‹è¯„ä¼°æ‘˜è¦")
        print("="*60)
        
        print(f"ğŸ¤– æ¨¡å‹è·¯å¾„: {self.model_path}")
        print(f"ğŸ“ˆ è¯„ä¼°æ ·æœ¬æ•°: {summary.get('total_samples', 'N/A')}")
        print(f"ğŸ“Š æŒ‡æ ‡çŠ¶æ€: {'å®Œæ•´æŒ‡æ ‡' if metrics_available else 'åŸºç¡€æŒ‡æ ‡'}")
        print()
        
        if metrics_available:
            print("ğŸ¯ ç›¸ä¼¼åº¦æŒ‡æ ‡:")
            print(f"   BLEU Score: {summary.get('avg_bleu', 0):.4f} Â± {summary.get('std_bleu', 0):.4f}")
            print(f"   ROUGE-1:    {summary.get('avg_rouge1', 0):.4f} Â± {summary.get('std_rouge1', 0):.4f}")
            print(f"   ROUGE-2:    {summary.get('avg_rouge2', 0):.4f} Â± {summary.get('std_rouge2', 0):.4f}")
            print(f"   ROUGE-L:    {summary.get('avg_rougeL', 0):.4f} Â± {summary.get('std_rougeL', 0):.4f}")
        else:
            print("ğŸ¯ åŸºç¡€ç›¸ä¼¼åº¦æŒ‡æ ‡:")
            print(f"   Jaccardç›¸ä¼¼åº¦: {summary.get('avg_jaccard_similarity', 0):.4f}")
            print(f"   é‡å ç‡:        {summary.get('avg_overlap_ratio', 0):.4f}")
        
        print()
        print("â±ï¸  æ€§èƒ½æŒ‡æ ‡:")
        print(f"   å¹³å‡ç”Ÿæˆæ—¶é—´: {summary.get('avg_generation_time', 0):.2f}ç§’")
        print(f"   å¹³å‡å“åº”é•¿åº¦: {summary.get('avg_response_length', 0):.1f}å­—ç¬¦")
        print()
        
        print("ğŸ“ æ ¼å¼æ­£ç¡®ç‡:")
        print(f"   æ¨ç†æ ¼å¼æ­£ç¡®ç‡: {summary.get('reasoning_format_rate', 0)*100:.1f}%")
        print(f"   ç­”æ¡ˆæ ¼å¼æ­£ç¡®ç‡: {summary.get('answer_format_rate', 0)*100:.1f}%")
        print(f"   ç©ºå“åº”ç‡:       {summary.get('empty_response_rate', 0)*100:.1f}%")
        print("="*60)

def main():
    """ä¸»å‡½æ•°"""
    # é…ç½®å‚æ•° - æ›´æ–°è·¯å¾„
    MODEL_PATH = "./models/deepseek_r1_1.5b_lora/best_model"  # è¦è¯„ä¼°çš„æ¨¡å‹è·¯å¾„
    TEST_FILE = "./dataset/sft_r1_val.jsonl"  # æµ‹è¯•æ•°æ®æ–‡ä»¶
    OUTPUT_DIR = "./scripts/compare/evaluation_results"  # è¾“å‡ºç›®å½•
    NUM_SAMPLES = 10  # è¯„ä¼°æ ·æœ¬æ•°é‡ (Noneè¡¨ç¤ºå…¨éƒ¨)
    
    # æ£€æŸ¥æµ‹è¯•æ–‡ä»¶æ˜¯å¦å­˜åœ¨
    if not os.path.exists(TEST_FILE):
        print(f"âŒ æµ‹è¯•æ–‡ä»¶ä¸å­˜åœ¨: {TEST_FILE}")
        print("è¯·æ£€æŸ¥æ–‡ä»¶è·¯å¾„æ˜¯å¦æ­£ç¡®")
        return
    
    # æ£€æŸ¥è¾“å‡ºç›®å½•æ˜¯å¦å­˜åœ¨ï¼Œä¸å­˜åœ¨åˆ™åˆ›å»º
    os.makedirs(OUTPUT_DIR, exist_ok=True)
    
    # å•ä¸ªæ¨¡å‹è¯„ä¼°
    print("å¼€å§‹å•ä¸ªæ¨¡å‹è¯„ä¼°...")
    try:
        evaluator = ModelEvaluator(MODEL_PATH)
        evaluation_result = evaluator.run_comprehensive_evaluation(TEST_FILE, OUTPUT_DIR, NUM_SAMPLES)
        
        # æ˜¾ç¤ºå‡ ä¸ªç¤ºä¾‹
        print("\nğŸ” ç¤ºä¾‹è¾“å‡º:")
        for i, result in enumerate(evaluation_result["results"][:3]):
            print(f"\nç¤ºä¾‹ {i+1}:")
            print(f"é—®é¢˜: {result['question']}")
            print(f"ç”Ÿæˆå›å¤: {result['generated_response']}")
            print(f"æ¨ç†éƒ¨åˆ†: {result['reasoning']}")
            print(f"ç­”æ¡ˆéƒ¨åˆ†: {result['answer']}")
            print("-" * 50)
            
    except Exception as e:
        print(f"è¯„ä¼°å¤±è´¥: {e}")
        import traceback
        traceback.print_exc()

if __name__ == "__main__":
    main()